# uav-floodnet-ss
This is the official repository for the paper "Leveraging Self-Supervised Features for Efficient Flooded Region Identification in UAV Aerial Images". The paper is under review and more details will be shared upon acceptance. 

# Abstract
Identifying regions affected by disasters is a vital step in effectively managing and planning relief and rescue efforts. Unlike the traditional approaches of manually assessing post-disaster damage, analyzing images of Unmanned Aerial Vehicles (UAVs) offers an objective and reliable way to assess the damage. In the past, segmentation techniques have been adopted to identify post-flood damage in UAV aerial images. However, most of these supervised learning approaches rely on manually annotated datasets. Indeed, annotating images is a time-consuming and error-prone task that requires domain expertise. This work focuses on leveraging self-supervised features to accurately identify flooded regions in UAV aerial images. This work proposes two encoder-decoder-based segmentation approaches, which integrate the visual features learned from DINOv2 with the traditional encoder backbone. This study investigates the generalization of self-supervised features for UAV aerial images. Specifically, we evaluate the effectiveness of features from the DINOv2 model, trained on non-aerial images, for segmenting aerial images, noting the distinct perspectives between the two image types. Our results demonstrate that DINOv2's self-supervised pretraining on natural images generates transferable, general-purpose visual features that streamline the development of aerial segmentation workflows. By leveraging these features as a foundation, we significantly reduce reliance on labor-intensive manual annotation processes, enabling high-accuracy segmentation with limited labeled aerial data.

# Usage
## Load Data
In this work, we utilized the [Floodnet (Supervised) v1.0 dataset](https://github.com/BinaLab/FloodNet-Supervised_v1.0) for the analysis of our models. After downloading the dataset and setting up the workspace, the overall directory structure is organized under a root folder named **Floodnet**, with the following subdirectories:
1. **dataset/**<br>
   This folder contains all the data used for training, validation, and testing. It is further organized into three subsets:<br>
   (a) **train/** - Training data<br>
   (b) **val/** - Validation data<br>
   (c) **test/** - Testing data<br>
   
2. **figures/**<br>
   This folder contains all the visualization outputs generated during model inference. It is divided by model type:<br>
   (a) **DINOv2_deeplabv3/** — Outputs (segmented images) generated by the DINOv2 + DeepLabV3 model<br>
   (b) **DINOv2_unet/** — Outputs (segmented images) generated by the DINOv2 + U-Net model<br>

3. **saved_models/**<br>
   All model weights and checkpoints from training runs are stored here for reproducibility or future fine-tuning<br>
   
The overview of the directory:<br>
<pre>
Floodnet
├── dataset
│   ├── train
│   │   ├── train-label-img     # Ground truth segmentation masks
│   │   └── train-org-img       # Original UAV RGB images
│   ├── val
│   │   ├── val-label-img       # Ground truth segmentation masks
│   │   └── val-org-img         # Original UAV RGB images
│   └── test
│       ├── test-label-img      # Ground truth segmentation masks
│       └── test-org-img        # Original UAV RGB images
├── figures
│   ├── DINOv2_deeplabv3        # Visual results from DINOv2 + DeepLabV3
│   └── DINOv2_unet             # Visual results from DINOv2 + U-Net
└── saved_models
    └── (Trained model weights and checkpoints)
</pre>
## Execution
To run the training and testing pipelines for this project, follow the structure below.<br>

1. **Training and Validation** <br>
(a) **dinov2_deeplabv3_train.py** - Trains and validates using DINOv2 + DeepLabV3<br>
(b) **dinov2_unet_train.py** - Trains and validates using DINOv2 + U-Net<br>

2. **Testing and Evaluation** <br>
(a) **dinov2_deeplabv3_test.py** - Generates output segmented images using DINOv2 + DeepLabV3 and returns classwise Intersection over Union (IoU) and mIoU<br>
(b) **dinov2_unet_test.py** - Generates output segmented images using DINOv2 + U-Net and returns classwise Intersection over Union (IoU) and mIoU<br>

All these scripts internally load the dataset and apply loss functions using **dataset_load.py** and **loss.py**

# Support
Feel free to contact: dibyabhadeb@gmail.com

 
